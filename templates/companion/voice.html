{% extends "base.html" %}

{% block title %}AI Companion{% endblock %}

{% block content %}
<div class="flex flex-col min-h-screen bg-gradient-to-b from-gray-50 to-white">
    <!-- Main Content Container -->
    <div class="flex-1 flex flex-col w-full max-w-5xl mx-auto px-4 md:px-6 lg:px-8 py-4 md:py-6 lg:py-8">
        <!-- Chat Container -->
        <div class="bg-white border border-gray-200/75 rounded-2xl shadow-sm p-4 md:p-6 lg:p-8 transition-all duration-300 ease-in-out transform hover:shadow-md mt-4 md:mt-6">
            <div class="flex flex-col h-full">
                <div class="flex items-center gap-3 mb-4">
                    <div class="w-10 h-10 rounded-full bg-primary-100 flex items-center justify-center">
                        <svg class="w-6 h-6 text-primary-600" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                        </svg>
                    </div>
                    <div>
                        <h2 class="text-lg font-semibold text-gray-900">Voice Chat</h2>
                        <p class="text-sm text-gray-500">Have a natural conversation with Synthia</p>
                    </div>
                </div>
                <div class="flex-1 overflow-y-auto" id="chatMessages">
                    <!-- Messages will be inserted here -->
                </div>
                <div class="mt-4">
                    <button id="micButton" 
                            class="inline-flex items-center justify-center w-full px-5 py-3 bg-primary-600 text-white text-sm font-medium rounded-xl shadow-sm hover:bg-primary-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary-500 transition-all duration-200 group">
                        <svg class="w-6 h-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                        </svg>
                        Start Recording
                    </button>
                    <span id="status" class="text-sm text-gray-500 italic"></span>
                </div>
            </div>
        </div>
    </div>
</div>

<style nonce="{{ get_csp_nonce() }}">
/* Animations */
@keyframes card-hover {
    0% {
        transform: translateY(0);
    }
    100% {
        transform: translateY(-4px);
    }
}

/* Ensure proper container heights */
@media screen and (min-height: 700px) {
    .flex-1 {
        min-height: 500px;
    }
}

/* Mobile optimizations */
@media screen and (max-width: 640px) {
    .grid {
        gap: 1rem;
    }
    
    .rounded-2xl {
        border-radius: 1rem;
    }
}

/* Hover effects */
.hover\:shadow-md:hover {
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
    animation: card-hover 0.3s ease forwards;
}
</style>
{% endblock %}

{% block scripts %}
<script nonce="{{ get_csp_nonce() }}">
let isRecording = false;
let mediaRecorder = null;
let speechSynthesis = window.speechSynthesis;
let voices = [];
let recognition = null;
let audioContext = null;
let analyser = null;
let silenceStart = null;
let silenceThreshold = -65;
let silenceDuration = 2000;
let audioDetectionInterval = null;
let currentThreadId = null;

// Initialize on page load
document.addEventListener('DOMContentLoaded', function() {
    // Load existing thread or create new one
    currentThreadId = sessionStorage.getItem('threadId');
    if (!currentThreadId) {
        createNewThread();
    } else {
        fetchChatHistory(currentThreadId);
    }
    
    // Setup event listeners
    document.getElementById('micButton').addEventListener('click', toggleRecording);
    
    // Load voices for speech synthesis
    loadVoices();
});

function loadVoices() {
    voices = speechSynthesis.getVoices();
}

if (speechSynthesis.onvoiceschanged !== undefined) {
    speechSynthesis.onvoiceschanged = loadVoices;
}

function createNewThread() {
    currentThreadId = uuidv4();
    sessionStorage.setItem('threadId', currentThreadId);
    document.getElementById('chatMessages').innerHTML = '';
    fetchChatHistory(currentThreadId);
}

function fetchChatHistory(threadId = null) {
    if (!threadId) return;
    
    fetch('/companion/history?thread_id=' + threadId)
        .then(response => response.json())
        .then(data => {
            if (data.messages) {
                document.getElementById('chatMessages').innerHTML = '';
                data.messages.forEach(msg => {
                    addMessage(msg.content, msg.type);
                });
            }
        })
        .catch(error => {
            showError('Failed to load chat history: ' + error.message);
        });
}

async function sendMessageToAI(message) {
    try {
        // Add user message to UI immediately
        addMessage(message, 'user');
        
        const response = await fetch('/companion/chat', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                message: message,
                request_id: uuidv4(),
                type: 'voice',
                thread_id: currentThreadId
            })
        });
        
        const data = await response.json();
        
        if (data.success) {
            addMessage(data.response, 'ai');
            speakMessage(data.response);
        } else {
            throw new Error(data.error || 'Failed to get AI response');
        }
    } catch (error) {
        showError(error.message);
    }
}

function detectSilence(stream) {
    if (!audioContext) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
    }
    
    const source = audioContext.createMediaStreamSource(stream);
    analyser = audioContext.createAnalyser();
    analyser.fftSize = 2048;
    source.connect(analyser);

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Float32Array(bufferLength);

    audioDetectionInterval = setInterval(() => {
        analyser.getFloatFrequencyData(dataArray);
        
        // Calculate average volume level in dB
        const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;
        
        if (average < silenceThreshold) {
            if (!silenceStart) {
                silenceStart = Date.now();
            } else if (Date.now() - silenceStart > silenceDuration) {
                console.log('Silence detected, stopping recording');
                stopRecording();
            }
        } else {
            silenceStart = null;
        }
    }, 100);
}

function stopRecording() {
    if (!isRecording) return;
    
    isRecording = false;
    const button = document.getElementById('micButton');
    const status = document.getElementById('status');

    button.classList.remove('bg-red-600', 'hover:bg-red-700');
    button.classList.add('bg-primary-600', 'hover:bg-primary-700');
    button.innerHTML = `
        <svg class="w-6 h-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
        </svg>
        Start Recording
    `;
    status.textContent = '';
    
    if (recognition) {
        recognition.stop();
    }

    if (audioDetectionInterval) {
        clearInterval(audioDetectionInterval);
        audioDetectionInterval = null;
    }

    if (audioContext) {
        if (analyser) {
            analyser.disconnect();
            analyser = null;
        }
    }

    silenceStart = null;
}

function toggleRecording() {
    const button = document.getElementById('micButton');
    const status = document.getElementById('status');
    
    if (!isRecording) {
        initializeSpeechRecognition();
        
        navigator.mediaDevices.getUserMedia({ 
            audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
            }
        })
        .then(stream => {
            isRecording = true;
            button.classList.add('bg-red-600', 'hover:bg-red-700');
            button.classList.remove('bg-primary-600', 'hover:bg-primary-700', 'opacity-50', 'cursor-not-allowed');
            button.innerHTML = `
                <svg class="w-6 h-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                </svg>
                Stop Recording
            `;
            status.textContent = 'Recording... Speak clearly into your microphone (will auto-stop after silence)';
            status.className = 'text-sm text-green-600 italic font-medium';
            
            recognition.start();
            detectSilence(stream);
        })
        .catch(error => {
            console.error('Error accessing microphone:', error);
            status.textContent = 'Error: Could not access microphone';
            status.className = 'text-sm text-red-600 italic font-medium';
            showError('Please ensure your microphone is connected and you have granted permission to use it.');
        });
    } else {
        stopRecording();
    }
}

function initializeSpeechRecognition() {
    if (!recognition) {
        recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.lang = 'en-US';
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.maxAlternatives = 3;

        let finalTranscript = '';

        recognition.onstart = () => {
            finalTranscript = '';
            console.log('Speech recognition started');
            document.getElementById('status').textContent = 'Listening... Click Stop when done speaking';
        };

        recognition.onresult = (event) => {
            let interimTranscript = '';
            
            for (let i = event.resultIndex; i < event.results.length; i++) {
                const transcript = event.results[i][0].transcript;
                if (event.results[i].isFinal) {
                    finalTranscript += transcript;
                } else {
                    interimTranscript += transcript;
                }
            }

            if (interimTranscript) {
                document.getElementById('status').textContent = 'âœ“ Heard: ' + interimTranscript;
                document.getElementById('status').className = 'text-sm text-green-600 italic font-medium';
            }
        };

        recognition.onerror = (event) => {
            console.error('Speech recognition error:', event.error);
            const errorMessages = {
                'no-speech': 'No speech detected. Please speak louder or check your microphone.',
                'aborted': 'Speech recognition was interrupted. Please try again.',
                'audio-capture': 'No microphone detected. Please check your microphone connection.',
                'network': 'Network error occurred. Please check your internet connection.',
                'not-allowed': 'Microphone permission denied. Please allow microphone access.',
                'service-not-allowed': 'Speech recognition service not available in your browser.',
                'bad-grammar': 'Speech recognition configuration error.',
                'language-not-supported': 'Selected language is not supported.'
            };
            const errorMessage = errorMessages[event.error] || `Speech recognition error: ${event.error}`;
            document.getElementById('status').textContent = errorMessage;
            document.getElementById('status').className = 'text-sm text-red-600 italic font-medium';
            if (event.error === 'not-allowed') {
                showError('Please allow microphone access in your browser settings to use voice chat.');
            }
        };

        recognition.onend = () => {
            if (finalTranscript) {
                addMessage(finalTranscript, 'user');
                sendMessageToAI(finalTranscript);
                finalTranscript = '';
            }
            console.log('Speech recognition ended');
        };
    }
}

function addMessage(message, type) {
    const chatMessages = document.getElementById('chatMessages');
    const messageDiv = document.createElement('div');
    messageDiv.className = 'flex items-start ' + (type === 'user' ? 'justify-end' : '');
    
    const content = `
        ${type === 'user' ? '' : `
        <div class="flex-shrink-0">
            <div class="h-10 w-10 rounded-full bg-primary-500 flex items-center justify-center">
                <svg class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z" />
                </svg>
            </div>
        </div>
        `}
        <div class="${type === 'user' ? 'mr-3' : 'ml-3'}">
            <div class="${type === 'user' ? 'bg-primary-600 text-white' : 'bg-white'} rounded-lg shadow-sm p-4 max-w-lg">
                <p class="${type === 'user' ? 'text-white' : 'text-gray-900'}">${typeof message === 'string' ? message : message.message}</p>
            </div>
        </div>
    `;
    
    messageDiv.innerHTML = content;
    chatMessages.appendChild(messageDiv);
    chatMessages.scrollTop = chatMessages.scrollHeight;
}

function speakMessage(message) {
    speechSynthesis.cancel();

    const utterance = new SpeechSynthesisUtterance(message);
    
    const englishVoices = voices.filter(voice => voice.lang.startsWith('en-'));
    if (englishVoices.length > 0) {
        const femaleVoice = englishVoices.find(voice => voice.name.includes('female') || voice.name.includes('Female'));
        utterance.voice = femaleVoice || englishVoices[0];
    }
    
    utterance.rate = 1.0;
    utterance.pitch = 1.0;
    utterance.volume = 1.0;
    
    speechSynthesis.speak(utterance);
}

function showError(message) {
    const errorDiv = document.createElement('div');
    errorDiv.className = 'flex items-start mb-4';
    errorDiv.innerHTML = `
        <div class="flex-shrink-0">
            <div class="h-10 w-10 rounded-full bg-red-500 flex items-center justify-center">
                <svg class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4m0 4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                </svg>
            </div>
        </div>
        <div class="ml-3">
            <div class="bg-red-50 text-red-700 rounded-lg shadow-sm p-4 max-w-lg">
                <p>${message}</p>
            </div>
        </div>
    `;
    
    const chatMessages = document.getElementById('chatMessages');
    chatMessages.appendChild(errorDiv);
    chatMessages.scrollTop = chatMessages.scrollHeight;
}

function uuidv4() {
    return ([1e7]+-1e3+-4e3+-8e3+-1e11).replace(/[018]/g, c =>
        (c ^ crypto.getRandomValues(new Uint8Array(1))[0] & 15 >> c / 4).toString(16)
    );
}
</script>
{% endblock %}